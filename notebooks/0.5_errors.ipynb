{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import control as ct\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "from speckit import compute_spectrum\n",
    "from speckit.plotting import default_rc\n",
    "\n",
    "plt.rcParams.update(default_rc)\n",
    "\n",
    "pool = mp.Pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the system transfer function\n",
    "tau = 100  # Time constant\n",
    "fs = 2  # Sampling frequency (Hz)\n",
    "dt = 1 / fs  # Time step\n",
    "n = int(2e6)  # Number of samples\n",
    "\n",
    "# Transfer function H(s) = 1 / (tau*s + 1)\n",
    "num = [1]\n",
    "den = [tau, 1]\n",
    "system = signal.TransferFunction(num, den)\n",
    "\n",
    "# Step 2: Generate the input signal (white noise)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "input_signal = np.random.normal(0, 1, n)\n",
    "\n",
    "# Step 3: Generate the output signal by filtering the input through the system\n",
    "t = np.linspace(0, n * dt, n)\n",
    "_, output_signal, _ = signal.lsim(system, input_signal, t)\n",
    "\n",
    "# Introduce frequency-dependent noise to the output\n",
    "freq = np.fft.fftfreq(n, d=dt)\n",
    "noise = np.fft.ifft(np.fft.fft(np.random.normal(0, 1, n)) * (np.abs(freq) ** 0.5)).real\n",
    "output_signal_noisy = output_signal + 0.5 * noise  # Scale noise appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(input_signal, label=\"input\")\n",
    "ax.plot(output_signal_noisy, label=\"output\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Signals\")\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute the cross-spectral density (Welch)\n",
    "f, Pxy = signal.csd(input_signal, output_signal_noisy, fs=fs, nperseg=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compute the cross-spectral density (spectools)\n",
    "ltf_obj = compute_spectrum(\n",
    "    [input_signal, output_signal_noisy], fs=fs, pool=pool, win=\"Kaiser\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "ax1.semilogx(f, ct.mag2db(np.abs(Pxy)), ls=\"--\", label=\"Welch\")\n",
    "ax2.semilogx(f, -np.angle(Pxy, deg=True), ls=\"--\")\n",
    "ax2.semilogx(ltf_obj.f, np.angle(ltf_obj.Gxy, deg=True))\n",
    "ax1.semilogx(ltf_obj.f, ct.mag2db(np.abs(ltf_obj.Gxy)), label=\"spectools\")\n",
    "ax2.set_xlabel(\"Frequency (Hz)\")\n",
    "ax1.set_ylabel(\"Magnitude (dB)\")\n",
    "ax2.set_ylabel(\"Phase (deg)\")\n",
    "ax1.set_title(\"Cross spectral density estimate\")\n",
    "ax1.legend(framealpha=1, edgecolor=\"k\")\n",
    "ax1.grid(ls=\"--\")\n",
    "ax2.grid(ls=\"--\")\n",
    "ax1.set_xlim(ltf_obj.f[0], ltf_obj.f[-1])\n",
    "ax2.set_xlim(ltf_obj.f[0], ltf_obj.f[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax2, ax1) = ltf_obj.plot(\"bode\", errors=True, sigma=3)\n",
    "ax1.set_ylim(-120, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(ltf_obj.f, ltf_obj.coh, label=\"Coherence\")\n",
    "ax.loglog(\n",
    "    ltf_obj.f, ltf_obj.coh_error, label=\"Normalized random error\", lw=6, c=\"tomato\"\n",
    ")\n",
    "ax.loglog(ltf_obj.f, ltf_obj.coh_dev, label=\"Standard deviation\", c=\"lime\")\n",
    "ax.loglog(ltf_obj.f, ltf_obj.coh_error * ltf_obj.coh, ls=\":\", c=\"k\")\n",
    "ax.set_xlabel(\"Fourier frequency (Hz)\")\n",
    "ax.set_ylabel(\"Coherence\")\n",
    "ax.legend(framealpha=1, edgecolor=\"k\")\n",
    "ax.set_xlim(ltf_obj.f[0], ltf_obj.f[-1])\n",
    "ax.set_ylim(1e-7, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(\n",
    "    ltf_obj.f, ltf_obj.coh_dev, lw=6, label=r\"$\\sigma[\\gamma_{xy}^2]$\", c=\"tomato\"\n",
    ")\n",
    "ax.loglog(\n",
    "    ltf_obj.f,\n",
    "    ltf_obj.coh_error * ltf_obj.coh,\n",
    "    label=r\"$ \\gamma_{xy}^2 \\cdot \\epsilon_r [\\gamma_{xy}^2]$\",\n",
    "    c=\"lime\",\n",
    ")\n",
    "ax.set_xlabel(\"Fourier frequency (Hz)\")\n",
    "ax.set_xlim(ltf_obj.f[0], ltf_obj.f[-1])\n",
    "ax.legend(edgecolor=\"k\", framealpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(ltf_obj.f, ltf_obj.Gxy_dev, lw=6, label=r\"$\\sigma[ |G_{xy}| ]$\", c=\"tomato\")\n",
    "ax.loglog(\n",
    "    ltf_obj.f,\n",
    "    ltf_obj.Gxy_error * np.abs(ltf_obj.Gxy),\n",
    "    label=r\"$|G_{xy}| \\cdot \\epsilon_r[|G_{xy}|]$\",\n",
    "    c=\"lime\",\n",
    ")\n",
    "ax.set_xlabel(\"Fourier frequency (Hz)\")\n",
    "ax.set_xlim(ltf_obj.f[0], ltf_obj.f[-1])\n",
    "ax.legend(edgecolor=\"k\", framealpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(ltf_obj.f, ltf_obj.Hxy_dev, lw=6, label=r\"$\\sigma[ |H_{xy}| ]$\", c=\"tomato\")\n",
    "ax.loglog(\n",
    "    ltf_obj.f,\n",
    "    ltf_obj.Hxy_mag_error * np.abs(ltf_obj.Hxy),\n",
    "    label=r\"$|H_{xy}| \\cdot \\epsilon_r[|H_{xy}|]$\",\n",
    "    c=\"lime\",\n",
    ")\n",
    "ax.set_xlabel(\"Fourier frequency (Hz)\")\n",
    "ax.set_xlim(ltf_obj.f[0], ltf_obj.f[-1])\n",
    "ax.legend(edgecolor=\"k\", framealpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(\n",
    "    ltf_obj.f,\n",
    "    ltf_obj.Hxy_rad_error,\n",
    "    lw=6,\n",
    "    label=r\"$\\epsilon_r[ arg[ H_{xy}] ]$\",\n",
    "    c=\"tomato\",\n",
    ")\n",
    "ax.loglog(\n",
    "    ltf_obj.f,\n",
    "    ltf_obj.Hxy_dev / ltf_obj.cf,\n",
    "    label=r\"$\\sigma[|H_{xy}|] / |H_{xy}|$\",\n",
    "    c=\"lime\",\n",
    ")\n",
    "ax.set_xlabel(\"Fourier frequency (Hz)\")\n",
    "ax.set_ylabel(r\"$\\sigma(\\gamma_{xy}^2)$\")\n",
    "ax.set_xlim(ltf_obj.f[0], ltf_obj.f[-1])\n",
    "ax.legend(edgecolor=\"k\", framealpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from speckit.analysis import compute_single_bin\n",
    "\n",
    "\n",
    "def run_monte_carlo_validation(\n",
    "    num_realizations: int = 1000,\n",
    "    fs: float = 100.0,\n",
    "    n_samples: int = 10000,\n",
    "    target_freq: float = 10.0,\n",
    "    segment_length: int = 256,\n",
    "):\n",
    "    \"\"\"\n",
    "    Validates the analytical error formulas in speckit using a Monte Carlo simulation.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Starting Monte Carlo Validation for Error Formulas\")\n",
    "    print(f\"Number of Realizations: {num_realizations}\")\n",
    "    print(f\"Signal Length: {n_samples} samples\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # --- Storage for the distribution of estimates ---\n",
    "    Gxx_estimates = []\n",
    "    coh_estimates = []\n",
    "\n",
    "    # --- Store the first analytical prediction ---\n",
    "    # (Theoretically, it should be similar for all realizations)\n",
    "    first_analytical_Gxx_dev = None\n",
    "    first_analytical_coh_dev = None\n",
    "\n",
    "    # --- Define the true signal properties ---\n",
    "    # Create two signals: one pure noise, one with a coherent sine wave + noise\n",
    "    # This gives us a known, non-trivial coherence value.\n",
    "    true_coh_amplitude = 0.8  # Target coherence around 0.8^2 = 0.64\n",
    "    noise_std = 5.0\n",
    "\n",
    "    # Create a reproducible random number generator\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "\n",
    "    for i in tqdm(range(num_realizations), desc=\"Running Realizations\"):\n",
    "        # --- 1. Generate a new, independent realization of the signals ---\n",
    "        t = np.arange(n_samples) / fs\n",
    "\n",
    "        # Common coherent part\n",
    "        common_signal = true_coh_amplitude * np.sin(2 * np.pi * target_freq * t)\n",
    "\n",
    "        # Independent noise parts\n",
    "        noise1 = rng.normal(loc=0, scale=noise_std, size=n_samples)\n",
    "        noise2 = rng.normal(loc=0, scale=noise_std, size=n_samples)\n",
    "\n",
    "        x = common_signal + noise1\n",
    "        y = common_signal + noise2\n",
    "\n",
    "        # --- 2. Compute the spectral estimates for this realization ---\n",
    "        result = compute_single_bin(\n",
    "            data=[x, y], fs=fs, freq=target_freq, L=segment_length, win=\"hann\", olap=0.5\n",
    "        )\n",
    "\n",
    "        # --- 3. Store the estimates ---\n",
    "        Gxx_estimates.append(result.Gxx[0])\n",
    "        coh_estimates.append(result.coh[0])\n",
    "\n",
    "        # --- 4. Store the first analytical prediction for comparison ---\n",
    "        if i == 0:\n",
    "            first_analytical_Gxx_dev = result.Gxx_dev[0]\n",
    "            first_analytical_coh_dev = result.coh_dev[0]\n",
    "            print(\n",
    "                f\"\\nSingle-run analytical prediction for Gxx_dev: {first_analytical_Gxx_dev:.4e}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Single-run analytical prediction for coh_dev: {first_analytical_coh_dev:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "    # --- 5. Analyze the distribution of estimates ---\n",
    "    Gxx_estimates = np.array(Gxx_estimates)\n",
    "    coh_estimates = np.array(coh_estimates)\n",
    "\n",
    "    # Measured standard deviation from the Monte Carlo simulation\n",
    "    measured_Gxx_dev = np.std(Gxx_estimates)\n",
    "    measured_coh_dev = np.std(coh_estimates)\n",
    "\n",
    "    print(\"\\n--- Validation Results ---\")\n",
    "    print(f\"Mean Gxx estimate: {np.mean(Gxx_estimates):.4e}\")\n",
    "    print(f\"Measured Gxx_dev (from simulation): {measured_Gxx_dev:.4e}\")\n",
    "    print(f\"Predicted Gxx_dev (from speckit):  {first_analytical_Gxx_dev:.4e}\")\n",
    "\n",
    "    gxx_rel_error = (\n",
    "        np.abs(measured_Gxx_dev - first_analytical_Gxx_dev) / measured_Gxx_dev\n",
    "    )\n",
    "    print(f\"--> Relative Difference: {gxx_rel_error:.2%}\\n\")\n",
    "\n",
    "    print(f\"Mean coherence estimate: {np.mean(coh_estimates):.4f}\")\n",
    "    print(f\"Measured coh_dev (from simulation): {measured_coh_dev:.4f}\")\n",
    "    print(f\"Predicted coh_dev (from speckit):  {first_analytical_coh_dev:.4f}\")\n",
    "\n",
    "    coh_rel_error = (\n",
    "        np.abs(measured_coh_dev - first_analytical_coh_dev) / measured_coh_dev\n",
    "    )\n",
    "    print(f\"--> Relative Difference: {coh_rel_error:.2%}\\n\")\n",
    "\n",
    "    # A successful test should have a relative difference of < 10-15%\n",
    "    # (The agreement improves with more realizations and more averages per estimate)\n",
    "    if gxx_rel_error < 0.15 and coh_rel_error < 0.15:\n",
    "        print(\"SUCCESS: Analytical predictions match simulation results.\")\n",
    "        # This would raise our confidence to ~99%\n",
    "    else:\n",
    "        print(\"WARNING: Discrepancy detected between analytical and simulated errors.\")\n",
    "\n",
    "    # --- 6. Plot the distributions for visual inspection ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    ax1.hist(Gxx_estimates, bins=30, density=True, edgecolor=\"k\", alpha=0.7)\n",
    "    ax1.set_title(f\"Distribution of Gxx Estimates (N={num_realizations})\")\n",
    "    ax1.set_xlabel(\"Gxx Value\")\n",
    "    ax1.axvline(\n",
    "        np.mean(Gxx_estimates),\n",
    "        color=\"r\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean = {np.mean(Gxx_estimates):.2e}\",\n",
    "    )\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.hist(coh_estimates, bins=30, density=True, edgecolor=\"k\", alpha=0.7)\n",
    "    ax2.set_title(f\"Distribution of Coherence Estimates (N={num_realizations})\")\n",
    "    ax2.set_xlabel(\"Coherence Value\")\n",
    "    ax2.axvline(\n",
    "        np.mean(coh_estimates),\n",
    "        color=\"r\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean = {np.mean(coh_estimates):.2f}\",\n",
    "    )\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_monte_carlo_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
